{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:00:20.380351Z",
     "start_time": "2025-11-12T13:00:20.377926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pipeline import PipelineConfig, PipelineFactory"
   ],
   "id": "e9ea8aeba5ba1d8c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:00:20.485515Z",
     "start_time": "2025-11-12T13:00:20.471661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load\n",
    "df = pd.read_csv(\"../data/cleaned_data_after_eda.csv\")\n",
    "TARGET = \"Percent_Bleached\"\n",
    "y = df[TARGET].astype(int)\n",
    "X = df.drop(columns=[TARGET])\n",
    "\n",
    "# Column lists\n",
    "numerical_cols    = X.select_dtypes(include=\"number\").columns.tolist()\n",
    "categorical_cols  = [\"Exposure\"]  # only this one is categorical per your note\n",
    "log_cols          = []            # fill if you want log1p on specific numeric cols\n"
   ],
   "id": "1d685ba6b07e2c40",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:00:20.837646Z",
     "start_time": "2025-11-12T13:00:20.562229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pipeline.weights import WeightingConfig\n",
    "\n",
    "# Split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "# Build config\n",
    "cfg = PipelineConfig(\n",
    "    numeric=numerical_cols,\n",
    "    categorical=categorical_cols,\n",
    "    log=log_cols,\n",
    "    model_name=\"hgbr\",                 # just this change\n",
    "    # weighting=WeightingConfig(                   # mild tail emphasis\n",
    "    #     scheme=\"tail_focus\", tails=0.10, power=1.2, normalize=True\n",
    "    # ),\n",
    ")\n",
    "# Build & train pipeline\n",
    "pipe = PipelineFactory().build(cfg)\n",
    "pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# Evaluate (simple R^2 example)\n",
    "r2 = pipe.score(X_te, y_te)\n",
    "print(f\"Test R^2: {r2:.3f}\")\n"
   ],
   "id": "2a24af9b49a04432",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'HistGradientBoostingRegressor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m      9\u001B[39m cfg = PipelineConfig(\n\u001B[32m     10\u001B[39m     numeric=numerical_cols,\n\u001B[32m     11\u001B[39m     categorical=categorical_cols,\n\u001B[32m   (...)\u001B[39m\u001B[32m     16\u001B[39m     \u001B[38;5;66;03m# ),\u001B[39;00m\n\u001B[32m     17\u001B[39m )\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Build & train pipeline\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m pipe = \u001B[43mPipelineFactory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m pipe.fit(X_tr, y_tr)\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# Evaluate (simple R^2 example)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/HackTheFuture/src/pipeline/pipeline_factory.py:44\u001B[39m, in \u001B[36mPipelineFactory.build\u001B[39m\u001B[34m(self, cfg)\u001B[39m\n\u001B[32m     41\u001B[39m             steps.append((\u001B[33m\"\u001B[39m\u001B[33mresample\u001B[39m\u001B[33m\"\u001B[39m, sampler))\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cfg.model_name:\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m     base = \u001B[43mmake_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregistry\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel_registry\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     45\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(cfg, \u001B[33m\"\u001B[39m\u001B[33mweighting\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     46\u001B[39m         model = WeightedModel(base_estimator=base, weighting=cfg.weighting)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/HackTheFuture/src/pipeline/models.py:78\u001B[39m, in \u001B[36mmake_model\u001B[39m\u001B[34m(name, registry)\u001B[39m\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m reg:\n\u001B[32m     77\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnknown model \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m. Available: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(reg)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m clone(\u001B[43mreg\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mTypeError\u001B[39m: 'HistGradientBoostingRegressor' object is not callable"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:00:20.849303490Z",
     "start_time": "2025-11-12T12:59:28.070593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    q90 = np.quantile(y_true, 0.9)\n",
    "    mask = y_true >= q90\n",
    "    mae_top = mean_absolute_error(y_true[mask], y_pred[mask]) if mask.any() else np.nan\n",
    "    return {\"R2\": r2, \"MAE\": mae, \"MAE_top10%\": mae_top}\n",
    "\n",
    "yhat = pipe.predict(X_te)\n",
    "print(evaluate(y_te, yhat))"
   ],
   "id": "f05578bb0ebc6055",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R2': -0.23071004667245676, 'MAE': 19.051841746248293, 'MAE_top10%': 71.89189189189189}\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
