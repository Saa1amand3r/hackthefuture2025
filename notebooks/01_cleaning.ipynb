{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# To Bleach or Not to Bleach? \n",
    "\n",
    "## Quickstart Notebook\n",
    "\n",
    "**Goal:** identify drivers of coral bleaching and predict the percentage of coral that is bleached (`Percent_Bleached`).  \n",
    "**Dataset:** csv file with site-level (aka \"location\"/coral reef) grouping by `Site_ID`.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "*Oh no, the data seems to be corrupted!* It looks like the people from the fossil fuel industry have been tampering with the data. There are some missing values, some outliers, and invalid/duplicate rows. **Start by cleaning it up!**\n"
   ],
   "id": "f843131c44c80d92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "6076681a996e5815"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:15:54.011422Z",
     "start_time": "2025-11-12T10:15:53.865360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from typing import Iterable, Dict, Tuple, Literal\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "d8e175f886f46d4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reusable functions",
   "id": "7722073d84429d2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:15:56.670667Z",
     "start_time": "2025-11-12T10:15:56.659002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Strategy = Literal[\"impute\", \"drop\"]\n",
    "\n",
    "def iqr_mask(df: pd.DataFrame, columns: Iterable[str], k: float = 1.5) -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Return a boolean mask per selected column: True where value is an IQR outlier.\n",
    "    k=1.5 -> Tukey 'mild' outliers; use 3.0 for 'extreme'.\n",
    "    \"\"\"\n",
    "    masks: Dict[str, pd.Series] = {}\n",
    "    for col in columns:\n",
    "        s = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        q1 = s.quantile(0.25)\n",
    "        q3 = s.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        if pd.isna(iqr) or iqr == 0:\n",
    "            masks[col] = pd.Series(False, index=df.index)\n",
    "        else:\n",
    "            lower = q1 - k * iqr\n",
    "            upper = q3 + k * iqr\n",
    "            masks[col] = (s < lower) | (s > upper)\n",
    "    return masks\n",
    "\n",
    "def clean_outliers(\n",
    "    df: pd.DataFrame,\n",
    "    outlier_columns: Iterable[str],\n",
    "    *,\n",
    "    k_iqr: float = 1.5,\n",
    "    strategy: Strategy = \"impute\",\n",
    "    n_neighbors: int = 5,\n",
    "    weights: str = \"distance\",\n",
    "    output_csv: str | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "      1) Mark IQR outliers in `outlier_columns` -> NaN\n",
    "      2) strategy == 'impute': KNN-impute numeric columns (scaled)\n",
    "         strategy == 'drop'  : drop rows having NaN in any of `outlier_columns`\n",
    "      3) Drop any remaining NaNs (whole-row drop)\n",
    "    Returns: (clean_df, outlier_row_mask_before_cleaning)\n",
    "    \"\"\"\n",
    "    df_work = df.copy()\n",
    "\n",
    "    # 1) Detect outliers & set to NaN (only in requested columns)\n",
    "    masks = iqr_mask(df_work, outlier_columns, k=k_iqr)\n",
    "    mask_df = pd.DataFrame(masks, index=df_work.index) if masks else pd.DataFrame(index=df_work.index)\n",
    "    any_outlier = mask_df.any(axis=1) if not mask_df.empty else pd.Series(False, index=df_work.index)\n",
    "\n",
    "    for col, m in masks.items():\n",
    "        df_work.loc[m, col] = pd.NA\n",
    "\n",
    "    # 2) Clean by strategy\n",
    "    if strategy == \"impute\":\n",
    "        num_cols = df_work.select_dtypes(include=\"number\").columns.tolist()\n",
    "        if num_cols:\n",
    "            scaler = StandardScaler()\n",
    "            imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)\n",
    "\n",
    "            X_num = df_work[num_cols].astype(float).values\n",
    "            X_scaled = scaler.fit_transform(X_num)\n",
    "            X_imputed_scaled = imputer.fit_transform(X_scaled)\n",
    "            X_imputed = scaler.inverse_transform(X_imputed_scaled)\n",
    "\n",
    "            df_work[num_cols] = X_imputed\n",
    "    elif strategy == \"drop\":\n",
    "        df_work = df_work.dropna(subset=list(outlier_columns), how=\"any\")\n",
    "    else:\n",
    "        raise ValueError(\"strategy must be 'impute' or 'drop'\")\n",
    "\n",
    "    # 3) Drop any remaining NaNs anywhere (whole row)\n",
    "    df_clean = df_work.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "    if output_csv:\n",
    "        df_clean.to_csv(output_csv, index=False)\n",
    "\n",
    "    return df_clean, any_outlier\n"
   ],
   "id": "76aa7921053b57f8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data cleaning\n",
    "\n",
    "Reading the csv"
   ],
   "id": "6245a29397238a28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:15:59.734934Z",
     "start_time": "2025-11-12T10:15:59.694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filepath_train = r\"../data/coral_students.csv\"\n",
    "df = pd.read_csv(filepath_train)\n",
    "\n",
    "df.head()"
   ],
   "id": "1d936fb21362b05e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Sample_ID  Site_ID  ClimSST  Temperature_Kelvin  Temperature_Mean  \\\n",
       "0  10274495.0  12082.0   301.65              303.50            299.79   \n",
       "1  10274496.0  12083.0   299.31              300.84            299.75   \n",
       "2  10274497.0  12084.0   300.56              302.65            299.81   \n",
       "3  10274498.0  12085.0   299.75              302.43            299.81   \n",
       "4  10274499.0  12086.0   297.65              295.69            299.81   \n",
       "\n",
       "   Temperature_Minimum  Temperature_Maximum  \\\n",
       "0               293.35               305.54   \n",
       "1               293.68               305.44   \n",
       "2               293.35               305.47   \n",
       "3               293.35               305.47   \n",
       "4               293.35               305.47   \n",
       "\n",
       "   Temperature_Kelvin_Standard_Deviation  Windspeed  SSTA  ...  TSA_DHW  \\\n",
       "0                                   2.52        2.0  0.49  ...     1.27   \n",
       "1                                   2.54        6.0 -0.42  ...     1.20   \n",
       "2                                   2.50        5.0  0.36  ...     2.71   \n",
       "3                                   2.50        7.0  0.54  ...     3.60   \n",
       "4                                   2.50        7.0 -0.91  ...     0.00   \n",
       "\n",
       "   TSA_DHW_Standard_Deviation  TSA_DHWMax  TSA_DHWMean  Depth_m  \\\n",
       "0                        0.74        6.05         0.22      8.3   \n",
       "1                        0.93       10.39         0.27     14.9   \n",
       "2                        0.83        7.18         0.23     10.7   \n",
       "3                        0.83        7.18         0.23      7.6   \n",
       "4                        0.83        7.18         0.23     10.0   \n",
       "\n",
       "   Distance_to_Shore  Exposure  Turbidity  Cyclone_Frequency  Percent_Bleached  \n",
       "0             8311.0   exposed     0.0586          56.583448              4.76  \n",
       "1            10747.0   exposed     0.0543          52.842523             21.88  \n",
       "2             9396.0   exposed     0.0571          56.583448             19.66  \n",
       "3             9408.0   exposed     0.0571          56.583448             28.03  \n",
       "4             9362.0   exposed     0.0571          56.583448              2.75  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>Site_ID</th>\n",
       "      <th>ClimSST</th>\n",
       "      <th>Temperature_Kelvin</th>\n",
       "      <th>Temperature_Mean</th>\n",
       "      <th>Temperature_Minimum</th>\n",
       "      <th>Temperature_Maximum</th>\n",
       "      <th>Temperature_Kelvin_Standard_Deviation</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>SSTA</th>\n",
       "      <th>...</th>\n",
       "      <th>TSA_DHW</th>\n",
       "      <th>TSA_DHW_Standard_Deviation</th>\n",
       "      <th>TSA_DHWMax</th>\n",
       "      <th>TSA_DHWMean</th>\n",
       "      <th>Depth_m</th>\n",
       "      <th>Distance_to_Shore</th>\n",
       "      <th>Exposure</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Cyclone_Frequency</th>\n",
       "      <th>Percent_Bleached</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10274495.0</td>\n",
       "      <td>12082.0</td>\n",
       "      <td>301.65</td>\n",
       "      <td>303.50</td>\n",
       "      <td>299.79</td>\n",
       "      <td>293.35</td>\n",
       "      <td>305.54</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.22</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8311.0</td>\n",
       "      <td>exposed</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>56.583448</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10274496.0</td>\n",
       "      <td>12083.0</td>\n",
       "      <td>299.31</td>\n",
       "      <td>300.84</td>\n",
       "      <td>299.75</td>\n",
       "      <td>293.68</td>\n",
       "      <td>305.44</td>\n",
       "      <td>2.54</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.93</td>\n",
       "      <td>10.39</td>\n",
       "      <td>0.27</td>\n",
       "      <td>14.9</td>\n",
       "      <td>10747.0</td>\n",
       "      <td>exposed</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>52.842523</td>\n",
       "      <td>21.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10274497.0</td>\n",
       "      <td>12084.0</td>\n",
       "      <td>300.56</td>\n",
       "      <td>302.65</td>\n",
       "      <td>299.81</td>\n",
       "      <td>293.35</td>\n",
       "      <td>305.47</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.83</td>\n",
       "      <td>7.18</td>\n",
       "      <td>0.23</td>\n",
       "      <td>10.7</td>\n",
       "      <td>9396.0</td>\n",
       "      <td>exposed</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>56.583448</td>\n",
       "      <td>19.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10274498.0</td>\n",
       "      <td>12085.0</td>\n",
       "      <td>299.75</td>\n",
       "      <td>302.43</td>\n",
       "      <td>299.81</td>\n",
       "      <td>293.35</td>\n",
       "      <td>305.47</td>\n",
       "      <td>2.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.83</td>\n",
       "      <td>7.18</td>\n",
       "      <td>0.23</td>\n",
       "      <td>7.6</td>\n",
       "      <td>9408.0</td>\n",
       "      <td>exposed</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>56.583448</td>\n",
       "      <td>28.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10274499.0</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>297.65</td>\n",
       "      <td>295.69</td>\n",
       "      <td>299.81</td>\n",
       "      <td>293.35</td>\n",
       "      <td>305.47</td>\n",
       "      <td>2.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>7.18</td>\n",
       "      <td>0.23</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9362.0</td>\n",
       "      <td>exposed</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>56.583448</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Checking the structure of data",
   "id": "553a03121d6bdfdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:16:02.239651Z",
     "start_time": "2025-11-12T10:16:02.233851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.shape)\n",
    "df.isna().sum()"
   ],
   "id": "7a6b36a9f550e8bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4446, 41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sample_ID                                  1\n",
       "Site_ID                                    1\n",
       "ClimSST                                    1\n",
       "Temperature_Kelvin                         1\n",
       "Temperature_Mean                           1\n",
       "Temperature_Minimum                        1\n",
       "Temperature_Maximum                        1\n",
       "Temperature_Kelvin_Standard_Deviation      1\n",
       "Windspeed                                  1\n",
       "SSTA                                       1\n",
       "SSTA_Standard_Deviation                    1\n",
       "SSTA_Mean                                  1\n",
       "SSTA_Minimum                               1\n",
       "SSTA_Maximum                               1\n",
       "SSTA_Frequency                             1\n",
       "SSTA_Frequency_Standard_Deviation          1\n",
       "SSTA_FrequencyMax                          1\n",
       "SSTA_FrequencyMean                         1\n",
       "SSTA_DHW                                   1\n",
       "SSTA_DHW_Standard_Deviation                1\n",
       "SSTA_DHWMax                                1\n",
       "SSTA_DHWMean                               1\n",
       "TSA                                        1\n",
       "TSA_Standard_Deviation                     1\n",
       "TSA_Minimum                                1\n",
       "TSA_Maximum                                1\n",
       "TSA_Mean                                   1\n",
       "TSA_Frequency                              1\n",
       "TSA_Frequency_Standard_Deviation           1\n",
       "TSA_FrequencyMax                           1\n",
       "TSA_FrequencyMean                          1\n",
       "TSA_DHW                                    1\n",
       "TSA_DHW_Standard_Deviation                 1\n",
       "TSA_DHWMax                                 1\n",
       "TSA_DHWMean                                1\n",
       "Depth_m                                  185\n",
       "Distance_to_Shore                          1\n",
       "Exposure                                   1\n",
       "Turbidity                                  1\n",
       "Cyclone_Frequency                          1\n",
       "Percent_Bleached                           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Searching for duplicate rows",
   "id": "24f668eee7c544f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:16:14.231562Z",
     "start_time": "2025-11-12T10:16:14.217907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "uniqueId = [\"Sample_ID\"]\n",
    "\n",
    "# 1) Exact row duplicates\n",
    "dups = df[df.duplicated(subset=uniqueId, keep=False)]          # all duplicate rows\n",
    "df_no_dups = df.drop_duplicates(subset=uniqueId, keep=\"first\") # keep first occurrence\n",
    "\n",
    "# 2) Duplicates by subset of columns (business key)\n",
    "dups_by_key = df[df.duplicated(subset=uniqueId, keep=False)]\n",
    "firsts = df.drop_duplicates(subset=uniqueId, keep=\"first\")\n",
    "\n",
    "# 3) Mark duplicates instead of dropping\n",
    "df[\"is_dup\"] = df.duplicated(subset=uniqueId, keep=\"first\")\n",
    "\n",
    "# 4) Count duplicate groups\n",
    "dup_counts = (df\n",
    "  .groupby(uniqueId, dropna=False)\n",
    "  .size()\n",
    "  .reset_index(name=\"count\")\n",
    "  .query(\"count > 1\"))\n",
    "print(dup_counts)"
   ],
   "id": "f1d64bfc50ca9a03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Sample_ID  count\n",
      "125   10274739.0      2\n",
      "311   10275110.0      2\n",
      "376   10275267.0      2\n",
      "918   10276398.0      2\n",
      "1236  10290633.0      2\n",
      "1267  10322434.0      2\n",
      "1424  10323868.0      2\n",
      "2231  10326707.0      2\n",
      "2246  10326723.0      2\n",
      "2263  10326760.0      2\n",
      "2267  10326765.0      2\n",
      "2310  10326841.0      2\n",
      "2318  10326849.0      2\n",
      "2570  10327590.0      2\n",
      "2672  10327888.0      2\n",
      "2749  10328138.0      2\n",
      "2830  10328411.0      2\n",
      "2838  10328421.0      2\n",
      "2878  10328513.0      2\n",
      "3206  10329203.0      2\n",
      "3365  10329491.0      2\n",
      "3552  10329892.0      2\n",
      "3629  10329992.0      2\n",
      "3634  10330006.0      2\n",
      "3856  10330530.0      2\n",
      "3866  10330547.0      4\n",
      "3878  10330569.0      2\n",
      "3953  10330776.0      4\n",
      "3994  10330878.0      4\n",
      "4227  10331344.0      2\n",
      "4368  10331601.0      2\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Removing duplicates",
   "id": "c683816ebd61a667"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:16:17.546219Z",
     "start_time": "2025-11-12T10:16:17.538967Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.drop_duplicates(subset=[\"Sample_ID\"], keep=\"first\").reset_index(drop=True)",
   "id": "3e680e79667993ad",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Searching for outliers",
   "id": "68b87154170d3462"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:16:18.779860Z",
     "start_time": "2025-11-12T10:16:18.759767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cols = [\n",
    "    \"Temperature_Kelvin\", \"Temperature_Mean\", \"Temperature_Minimum\",\n",
    "    \"Temperature_Maximum\", \"Temperature_Kelvin_Standard_Deviation\",\n",
    "    \"Windspeed\", \"Depth_m\", \"Distance_to_Shore\"\n",
    "]\n",
    "\n",
    "masks = iqr_mask(df, cols, k=1.5)\n",
    "\n",
    "df_flagged = df.copy()\n",
    "for col, m in masks.items():\n",
    "    df_flagged[f\"{col}_is_outlier\"] = m\n",
    "\n",
    "if masks:\n",
    "    df_flagged[\"any_outlier\"] = pd.concat(masks.values(), axis=1).any(axis=1)\n",
    "else:\n",
    "    df_flagged[\"any_outlier\"] = False\n",
    "\n",
    "outlier_rows = df_flagged[df_flagged[\"any_outlier\"]]\n",
    "print(outlier_rows.count())\n"
   ],
   "id": "9417c266d10f0730",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample_ID                                           590\n",
      "Site_ID                                             590\n",
      "ClimSST                                             590\n",
      "Temperature_Kelvin                                  590\n",
      "Temperature_Mean                                    590\n",
      "Temperature_Minimum                                 590\n",
      "Temperature_Maximum                                 590\n",
      "Temperature_Kelvin_Standard_Deviation               590\n",
      "Windspeed                                           590\n",
      "SSTA                                                590\n",
      "SSTA_Standard_Deviation                             590\n",
      "SSTA_Mean                                           590\n",
      "SSTA_Minimum                                        590\n",
      "SSTA_Maximum                                        590\n",
      "SSTA_Frequency                                      590\n",
      "SSTA_Frequency_Standard_Deviation                   590\n",
      "SSTA_FrequencyMax                                   590\n",
      "SSTA_FrequencyMean                                  590\n",
      "SSTA_DHW                                            590\n",
      "SSTA_DHW_Standard_Deviation                         590\n",
      "SSTA_DHWMax                                         590\n",
      "SSTA_DHWMean                                        590\n",
      "TSA                                                 590\n",
      "TSA_Standard_Deviation                              590\n",
      "TSA_Minimum                                         590\n",
      "TSA_Maximum                                         590\n",
      "TSA_Mean                                            590\n",
      "TSA_Frequency                                       590\n",
      "TSA_Frequency_Standard_Deviation                    590\n",
      "TSA_FrequencyMax                                    590\n",
      "TSA_FrequencyMean                                   590\n",
      "TSA_DHW                                             590\n",
      "TSA_DHW_Standard_Deviation                          590\n",
      "TSA_DHWMax                                          590\n",
      "TSA_DHWMean                                         590\n",
      "Depth_m                                             532\n",
      "Distance_to_Shore                                   590\n",
      "Exposure                                            590\n",
      "Turbidity                                           590\n",
      "Cyclone_Frequency                                   590\n",
      "Percent_Bleached                                    590\n",
      "is_dup                                              590\n",
      "Temperature_Kelvin_is_outlier                       590\n",
      "Temperature_Mean_is_outlier                         590\n",
      "Temperature_Minimum_is_outlier                      590\n",
      "Temperature_Maximum_is_outlier                      590\n",
      "Temperature_Kelvin_Standard_Deviation_is_outlier    590\n",
      "Windspeed_is_outlier                                590\n",
      "Depth_m_is_outlier                                  590\n",
      "Distance_to_Shore_is_outlier                        590\n",
      "any_outlier                                         590\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Removing the outliers and 'na' values. Saving to the csv file",
   "id": "dc9c4ca642d25221"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:16:22.283693Z",
     "start_time": "2025-11-12T10:16:22.181413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clean_df, outlier_rows = clean_outliers(\n",
    "    df,\n",
    "    cols,\n",
    "    k_iqr=1.5,\n",
    "    strategy=\"drop\",\n",
    "    output_csv=\"../data/cleaned_data_dropped.csv\",\n",
    ")\n",
    "clean_df.isna().sum()"
   ],
   "id": "237ec8bf14c207af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample_ID                                0\n",
       "Site_ID                                  0\n",
       "ClimSST                                  0\n",
       "Temperature_Kelvin                       0\n",
       "Temperature_Mean                         0\n",
       "Temperature_Minimum                      0\n",
       "Temperature_Maximum                      0\n",
       "Temperature_Kelvin_Standard_Deviation    0\n",
       "Windspeed                                0\n",
       "SSTA                                     0\n",
       "SSTA_Standard_Deviation                  0\n",
       "SSTA_Mean                                0\n",
       "SSTA_Minimum                             0\n",
       "SSTA_Maximum                             0\n",
       "SSTA_Frequency                           0\n",
       "SSTA_Frequency_Standard_Deviation        0\n",
       "SSTA_FrequencyMax                        0\n",
       "SSTA_FrequencyMean                       0\n",
       "SSTA_DHW                                 0\n",
       "SSTA_DHW_Standard_Deviation              0\n",
       "SSTA_DHWMax                              0\n",
       "SSTA_DHWMean                             0\n",
       "TSA                                      0\n",
       "TSA_Standard_Deviation                   0\n",
       "TSA_Minimum                              0\n",
       "TSA_Maximum                              0\n",
       "TSA_Mean                                 0\n",
       "TSA_Frequency                            0\n",
       "TSA_Frequency_Standard_Deviation         0\n",
       "TSA_FrequencyMax                         0\n",
       "TSA_FrequencyMean                        0\n",
       "TSA_DHW                                  0\n",
       "TSA_DHW_Standard_Deviation               0\n",
       "TSA_DHWMax                               0\n",
       "TSA_DHWMean                              0\n",
       "Depth_m                                  0\n",
       "Distance_to_Shore                        0\n",
       "Exposure                                 0\n",
       "Turbidity                                0\n",
       "Cyclone_Frequency                        0\n",
       "Percent_Bleached                         0\n",
       "is_dup                                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
